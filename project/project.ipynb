{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import chain\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "from multiprocessing.dummy import Lock as ThreadLock \n",
    "from multiprocessing.dummy import Value as ThreadValue\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import pymorphy2\n",
    "import functools\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import validation_curve, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pymorphy2\n",
    "import random\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "from nltk.corpus import stopwords\n",
    "import scipy\n",
    "import copy\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "# import gensim \n",
    "\n",
    "# model2=gensim.models.KeyedVectors.load(\"model.model\")\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stop_words = set(stopwords.words([\"russian\", \"english\"]))\n",
    "stemmerR = SnowballStemmer(\"russian\")\n",
    "stemmerE = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Перевод слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'машина'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "lit = set('abcdefghijklmnopqrstuvwxyz')\n",
    "\n",
    "def translate_yandex(text):\n",
    "    if text[0] in lit:\n",
    "        LANG='en-ru'\n",
    "    else:\n",
    "        return text\n",
    "    URL = 'https://translate.yandex.net/api/v1.5/tr.json/translate?'\n",
    "    KEY = 'trnsl.1.1.20160119T035517Z.50c6906978ef1961.08d0c5ada49017ed764c042723895ffab867be7a'\n",
    "    TEXT = text\n",
    " \n",
    "    r = requests.post(URL, data={'key': KEY, 'text': TEXT, 'lang': LANG})\n",
    "    return r.text[r.text.find('['):-3][2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Считывание заголовков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inp_for_titles(line):\n",
    "    data = line.split('\\t', 1)\n",
    "    doc_id = int(data[0])\n",
    "    if len(data[1]) == 1:\n",
    "        title = ''\n",
    "    else:\n",
    "        title = data[1]\n",
    "    doc_to_title[doc_id] = title[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026\n"
     ]
    }
   ],
   "source": [
    "doc_to_title = {}\n",
    "with codecs.open('docs_titles.tsv', \"r\", \"utf_8_sig\") as f:\n",
    "    next(f)\n",
    "    with ThreadPool(10) as pool:\n",
    "        pool.map(inp_for_titles, f)\n",
    "print(len(doc_to_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parser_for_titles(ID):\n",
    "    title = (doc_to_title[ID])\n",
    "    title = title.lower()\n",
    "    words = [stemmerR.stem(stemmerE.stem(word)) for word in re.sub('[^a-zа-я0-9]', ' ', title).split()\n",
    "         if not word in stop_words]\n",
    "    vec[ID] = ' '.join([word for word in words if not word in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = {}\n",
    "with ThreadPool(10) as pool:\n",
    "    pool.map(Parser_for_titles, list(range(1,28027)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Заголовки содержат очень много английских слов, которые мы хотели перевести. Стало хуже. Многие слова не имею перевода, так как являются странными названиями сайтов.\n",
    "\n",
    "2) LDA на заголовках выдавал полную чушь.\n",
    "\n",
    "3) Стемминг показал себя лучше лемматизации. Было использовано много видов лемматизации: Wordnet, pymorphy2, StenfordNLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Парсим тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def illustration(func):\n",
    "    \"\"\"\n",
    "    Распаралеливание выкачки страниц.\n",
    "    \"\"\"\n",
    "    mutex = ThreadLock()\n",
    "    n_thread = ThreadValue('i',0)\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **argv):\n",
    "        result = func(*args, **argv)\n",
    "        with mutex:\n",
    "            nonlocal n_thread\n",
    "            n_thread.value +=1\n",
    "            print(f\"\\r{n_thread.value} objects are processed...\",end ='',flush = True)\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@illustration\n",
    "def Parse(num):\n",
    "    lit=set('абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ')\n",
    "    file = 'texts/content/{}.dat'.format(num)\n",
    "    f = codecs.open(file, encoding='utf-8')\n",
    "    soup = BeautifulSoup(f, 'lxml')\n",
    "    cont = soup.find_all('div')\n",
    "    result = []\n",
    "    for i in cont:\n",
    "        a=i.get_text().strip()\n",
    "        res=a.split()\n",
    "        for i in res:\n",
    "            fl=1\n",
    "            for j in i:\n",
    "                if not(j in lit):\n",
    "                    fl=0\n",
    "                    break\n",
    "            if fl and not i.lower() in stop_words:\n",
    "                result.append(stemmerR.stem(i.lower()))\n",
    "    doc_to_text[num] = ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026 objects are processed..."
     ]
    }
   ],
   "source": [
    "doc_to_text = {}\n",
    "with ThreadPool(10) as pool: \n",
    "    pool.map(Parse, list(range(1,28027)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parser_for_texts_without_stemming (texts_dict):\n",
    "    words_dict = {}\n",
    "    for ID, text in tqdm(texts_dict.items()):\n",
    "        words_dict[ID] = ' '.join([word for word in text.split() if not word in stop_words])\n",
    "    return words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d39c4c2c0d493daa3ab9fd0cb84238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=28026), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_to_text = Parser_for_texts_without_stemming(doc_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28026, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_txt_list = list(doc_to_text.values())\n",
    "df = pd.DataFrame(data = {'txt': doc_txt_list}, index=doc_to_text.keys())\n",
    "df['ind']=df.index\n",
    "df = df.sort_values('ind')\n",
    "df = df.drop(['ind'], axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('text_result_plus_stemm.csv', encoding = 'utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_to_text = {}\n",
    "with codecs.open('texts/text_result_plus_stemm.csv', \"r\", 'utf_8_sig') as f:\n",
    "    next(f)\n",
    "    for line in tqdm(f):\n",
    "        data = line.strip().split(',', 1)\n",
    "        doc_to_text[int(data[0])] = data[1]\n",
    "\n",
    "\n",
    "print(len(doc_to_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Можно просто так.\n",
    "a = pd.read_csv('texts/text_result_plus_stemm.csv')\n",
    "doc_to_text={}\n",
    "for i in range(1, 28027):\n",
    "    doc_to_text[i]=a.txt[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Parser_for_text(text_dict):\n",
    "#     words_dict = {}\n",
    "#     for ID, text in tqdm(text_dict.items()):\n",
    "#         words_dict[ID] = text.split(' ')\n",
    "#     return words_dict\n",
    "\n",
    "# vec_txt = Parser_for_text(doc_to_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) LDA на текстах выдал полный бред\n",
    "\n",
    "2) Score для текстов в разы меньше\n",
    "\n",
    "3) также пробовали выделять заголовки h1, h2, ..., h6 на сайтах. Получалось плохо. Было много \"пустых\" заголовков.\n",
    "\n",
    "4) Пробовали использовать количество картинок, видео, скрипта для введения доп. фичей. Score только падал."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Фичи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) cos фичи\n",
    "\n",
    "2) jac фичи\n",
    "\n",
    "3) dbscan - поиск аномалий\n",
    "\n",
    "4) Seva\n",
    "\n",
    "5) Пытались вытащить много со страниц, но не помогло\n",
    "\n",
    "6) Пытались добавить семантические расстояния, но тоже слабовато"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Importer:\n",
    "\n",
    "    def __init__(self, doc_to_title, n_f, nfj=0):\n",
    "        doc_to_title[0] = ''\n",
    "        self.doc_to_vec = TfidfVectorizer().fit_transform([doc_to_title[i] for i in range(len(doc_to_title))])\n",
    "        dtt={}\n",
    "        for i in doc_to_title.keys():\n",
    "            dtt[i]=doc_to_title[i].split(' ')\n",
    "        self.doc = dtt\n",
    "        self.n_features = n_f\n",
    "        self.n_features_jac = nfj\n",
    "        self.extract_features = self.cosine\n",
    "   \n",
    "    def cosine(self, group):\n",
    "        \n",
    "        def jaccard(group):\n",
    "            X = np.empty(shape=(group.size, self.n_features_jac), dtype=np.float)\n",
    "            for i, doc_ID in enumerate(group):\n",
    "                dist = np.empty(shape=(group.size, ), dtype=np.float)\n",
    "                vec = set(self.doc[doc_ID])\n",
    "                for j, doc_j in enumerate(group):\n",
    "                    if doc_ID == doc_j:\n",
    "                        dist[j] = 0\n",
    "                    else:\n",
    "                        vec_j = set(self.doc[doc_j])\n",
    "                        intersec = len(vec.intersection(vec_j))\n",
    "                        if intersec > 0:\n",
    "                            dist[j] = intersec/(len(vec) + len(vec_j) - intersec)\n",
    "                        else:\n",
    "                            dist[j] = 0\n",
    "                X[i] = np.sort(dist)[-self.n_features_jac:]\n",
    "            return X\n",
    "    \n",
    "        n = (self.n_features)//2\n",
    "        X = np.empty(shape=(group.size, self.n_features), dtype=np.float)\n",
    "        for i, title in enumerate(pairwise_distances(self.doc_to_vec[group], metric='cosine')):\n",
    "            X[i, :n] = sorted(title)[1:n + 1]\n",
    "        X[:, n:] = np.mean(X[:, :n], axis=0)\n",
    "        X[:, :n] /= X[:, n:]\n",
    "        if (self.n_features_jac==0):\n",
    "            return X\n",
    "        X_jac = jaccard(group)\n",
    "        return np.hstack((X,X_jac))\n",
    "    \n",
    "           \n",
    "    def import_from(self, file):\n",
    "        data = pd.read_csv(file)\n",
    "        groups = data.groupby('group_id')\n",
    "        if 'target' in data.columns:\n",
    "            X = np.empty(shape=(data.shape[0], self.n_features + self.n_features_jac +1), dtype=np.float)\n",
    "            y = np.empty(shape=(data.shape[0], ), dtype=bool)\n",
    "            group_ids = np.empty(shape=(data.shape[0], ), dtype=int)\n",
    "            i = 0\n",
    "            for group_id, group_indx in groups.groups.items():\n",
    "                j = i + group_indx.size\n",
    "                group = data.iloc[group_indx]\n",
    "                group_ids[i:j] = group_id\n",
    "                clf = DBSCAN(0.495, metric=\"cosine\", min_samples=7)\n",
    "                res = clf.fit_predict(self.doc_to_vec[group.doc_id])\n",
    "                for t in range(len(res)):\n",
    "                    if res[t] == -1:\n",
    "                        res[t] = 0\n",
    "                    else:\n",
    "                        res[t] = 1\n",
    "                y[i:j] = group.target\n",
    "                X[i:j,:-1] = self.extract_features(group.doc_id)\n",
    "                X[i:j,-1] = res\n",
    "                i = j\n",
    " \n",
    "            return X, y, group_ids\n",
    "        else:\n",
    "            X = np.empty(shape=(data.shape[0], self.n_features + self.n_features_jac +1), dtype=np.float)\n",
    "            pair_ids = np.empty(shape=(data.shape[0], ), dtype=int)\n",
    " \n",
    "            i = 0\n",
    "            for group_id, group_indx in groups.groups.items():\n",
    "                j = i + group_indx.size\n",
    "                group = data.iloc[group_indx]\n",
    "                pair_ids[i:j] = group.pair_id\n",
    "                clf = DBSCAN(0.495, metric=\"cosine\", min_samples=7)\n",
    "                res = clf.fit_predict(self.doc_to_vec[group.doc_id])\n",
    "                for t in range(len(res)):\n",
    "                    if res[t] == -1:\n",
    "                        res[t] = 0\n",
    "                    else:\n",
    "                        res[t] = 1\n",
    "                X[i:j,:-1] = self.extract_features(group.doc_id)\n",
    "                X[i:j,-1] = res\n",
    "                i = j\n",
    " \n",
    "            return X, pair_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 15) (11690,) (11690,)\n",
      "(16627, 15) (16627,)\n",
      "(11690, 15) (16627, 15)\n"
     ]
    }
   ],
   "source": [
    "#СЕВА, СПАСБО ЗА ФИЧИ!\n",
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))\n",
    "    \n",
    "y_train_SEV = []\n",
    "X_train_SEV = []\n",
    "groups_train_SEV = []\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        y_train_SEV.append(target_id)\n",
    "        groups_train_SEV.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, target_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_train_SEV.append(sorted(all_dist, reverse=True)[0:15]    )\n",
    "X_train_SEV = np.array(X_train_SEV)\n",
    "y_train_SEV = np.array(y_train_SEV)\n",
    "groups_train_SEV = np.array(groups_train_SEV)\n",
    "print (X_train_SEV.shape, y_train_SEV.shape, groups_train_SEV.shape)\n",
    "\n",
    "test_data = pd.read_csv('test_groups.csv')\n",
    "testgroups_titledata = {}\n",
    "for i in range(len(test_data)):\n",
    "    new_doc = test_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in testgroups_titledata:\n",
    "        testgroups_titledata[doc_group] = []\n",
    "    testgroups_titledata[doc_group].append((doc_id, title))\n",
    "    \n",
    "X_test_SEV = []\n",
    "groups_test_SEV = []\n",
    "for new_group in testgroups_titledata:\n",
    "    docs = testgroups_titledata[new_group]\n",
    "    for k, (doc_id, title) in enumerate(docs):\n",
    "        groups_test_SEV.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_test_SEV.append(sorted(all_dist, reverse=True)[0:15]    )\n",
    "X_test_SEV = np.array(X_test_SEV)\n",
    "groups_test_SEV = np.array(groups_test_SEV)\n",
    "print (X_test_SEV.shape, groups_test_SEV.shape)\n",
    "\n",
    "scale_features_std = StandardScaler()\n",
    "X_train_SEV=scale_features_std.fit_transform(X_train_SEV)\n",
    "X_test_SEV=scale_features_std.fit_transform(X_test_SEV)\n",
    "print(X_train_SEV.shape, X_test_SEV.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Валидации (малая её часть)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Перебрали, наверное, все модели sklearn, Чуть меньше 40 моделей валидировали.\n",
    "\n",
    "2) Подбирали параметры модели и фичи к ней.\n",
    "\n",
    "3) СТранно, но обычное разбиение по фолдам - не очень хорошо себя показало. Лучше было рандомное разбиение по фолдам (shuffle=True). Хотя под конец уже начали сомневаться в этом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_gauss():\n",
    "    cos_f = range(2,30,2)\n",
    "    jac_f = range(0, 7)\n",
    "    dbscan_f = [0, 1]\n",
    "    SEV_f = [0, 15]\n",
    "    for c_f in tqdm(cos_f):\n",
    "        for j_f in jac_f:\n",
    "            for db_f in dbscan_f:\n",
    "                for S_f in SEV_f:\n",
    "                    importer = Importer(vec, c_f, j_f)\n",
    "                    X, y, groups_train = importer.import_from('train_groups.csv')\n",
    "                    scaler = StandardScaler()\n",
    "                    scaler.fit(X[:, :c_f+j_f])\n",
    "                    if db_f==1:\n",
    "                        lft = scaler.transform(X[:,:c_f+j_f])\n",
    "                        X = np.hstack((lft, X[:,c_f+j_f][:,np.newaxis]))\n",
    "                    else:\n",
    "                        X = scaler.transform(X[:,:c_f+j_f])\n",
    "                    X = np.hstack((X, X_train_SEV[:,:S_f]))\n",
    "                    model = GaussianProcessClassifier(n_jobs = -1)\n",
    "                    print([cross_val_score(model, X, y, cv=10,scoring = 'f1').mean(),c_f, j_f, db_f, S_f, X.shape])\n",
    "                    \n",
    "cross_val_gauss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_knn():\n",
    "    cos_f = range(2,30,2)\n",
    "    jac_f = range(0, 7)\n",
    "    dbscan_f = [0, 1]\n",
    "    SEV_f = [0, 15]\n",
    "    nebrs = range(20, 220, 5)\n",
    "    f1 = []\n",
    "    for c_f in tqdm(cos_f):\n",
    "        for j_f in jac_f:\n",
    "            for db_f in dbscan_f:\n",
    "                for S_f in SEV_f:\n",
    "                    importer = Importer(vec, c_f, j_f)\n",
    "                    X, y, groups_train = importer.import_from('train_groups.csv')\n",
    "                    scaler = StandardScaler()\n",
    "                    scaler.fit(X[:, :c_f+j_f])\n",
    "                    if db_f==1:\n",
    "                        lft = scaler.transform(X[:,:c_f+j_f])\n",
    "                        X = np.hstack((lft, X[:,c_f+j_f][:,np.newaxis]))\n",
    "                    else:\n",
    "                        X = scaler.transform(X[:,:c_f+j_f])\n",
    "                    X = np.hstack((X, X_train_SEV[:,:S_f]))\n",
    "                    for n in nebrs:\n",
    "                        model = KNeighborsClassifier(n)\n",
    "                        \n",
    "                        f1.append([cross_val_score(model, X, y, cv=10,scoring = 'f1', n_jobs=-1).mean(),c_f, j_f, db_f, S_f, X.shape, n])\n",
    "    return f1\n",
    "                    \n",
    "f = cross_val_knn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cros_val_neural(X,y,act = 'tanh',cv = 10):\n",
    "    n_hidden = np.arange(5,205,10)\n",
    "    n_layers = [1, 2]\n",
    "    n_s = 0\n",
    "    error = []\n",
    "    for n_l in n_layers:\n",
    "        for n_h in tqdm(n_hidden):\n",
    "            if(n_l == 1):\n",
    "                kf = KFold(n_splits=cv,shuffle=True)\n",
    "                model = MLPClassifier(max_iter=800, hidden_layer_sizes=(n_h,), random_state=1, activation=act)\n",
    "                e = cross_val_score(model, X, y, cv=kf,scoring = 'f1',n_jobs = -1).mean()\n",
    "                print(e,n_l,n_h,n_s)\n",
    "                error.append([e,n_l,n_h,n_s])\n",
    "            else:\n",
    "                for n_h_2 in n_hidden:\n",
    "                    kf = KFold(n_splits=cv,shuffle=True)\n",
    "                    model = MLPClassifier(max_iter=800, hidden_layer_sizes=(n_h,n_h_2), random_state=1, activation=act)\n",
    "                    e = cross_val_score(model, X, y, cv=kf,scoring = 'f1',n_jobs = -1).mean()\n",
    "                    print(e,n_l,n_h,n_h_2)\n",
    "                    error.append([e,n_l,n_h,n_h_2])\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.752874777765091 2 60 10, 0.7503708319809625 2 10 10 для relu\n",
    "#0.7557752644516544 2 10 80, 0.7550001382383368 2 15 10 для tanh\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "cc = 20\n",
    "cj = 5\n",
    "importer = Importer(vec, cc, cj)\n",
    "X_train, y_train, groups_train = importer.import_from('train_groups.csv')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[:,:cc+cj])\n",
    "X_train = scaler.transform(X_train[:,:cc+cj])\n",
    "err = cros_val_neural(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_train = pd.read_csv('train_groups.csv')\n",
    "# groups_train = data_train.groupby('group_id')\n",
    "# data_test = pd.read_csv('test_groups.csv')\n",
    "# groups_test = data_test.groupby('group_id')\n",
    "# groups = {}\n",
    "# for ID, group_indx in groups_train.groups.items():\n",
    "#     group = data_train.iloc[group_indx]\n",
    "#     groups[ID] = group.doc_id.values\n",
    "    \n",
    "# for ID, group_indx in groups_test.groups.items():\n",
    "#     group = data_test.iloc[group_indx]\n",
    "#     groups[ID] = group.doc_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def best_th(cls, X12, X3, X4, X56, X7, y, groups, k=5):\n",
    "#     space = np.linspace(0.1, 0.9, 25)\n",
    "#     result = np.zeros_like(space)\n",
    "#     for train, test in GroupKFold(n_splits=k).split(X12, y, groups):\n",
    "#         cls.fit(X12[train], X3[train], X4[train],X56[train],X7[train], y[train])\n",
    "#         predict = cls.predict(X12[test], X3[test], X4[test], X56[test], X7[test])\n",
    "        \n",
    "#         for i, th in enumerate(space):\n",
    "#             result[i] += f1_score(y[test], predict > th)\n",
    "\n",
    "#     best = np.argmax(result)\n",
    "#     return space[best], result[best] / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "importer = Importer(vec, 8, 3)\n",
    "X12, y, groups_train = importer.import_from('train_groups.csv')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X12[:,:11])\n",
    "lft = scaler.transform(X12[:,:11])\n",
    "X12 = np.hstack((lft, X12[:,11][:,np.newaxis]))\n",
    "\n",
    "importer = Importer(vec, 26, 4)\n",
    "X3, y, groups_train = importer.import_from('train_groups.csv')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X3[:,:30])\n",
    "X3 = scaler.transform(X3[:,:30])\n",
    "\n",
    "importer = Importer(vec, 8, 3)\n",
    "X4, y, groups_train = importer.import_from('train_groups.csv')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X4[:,:11])\n",
    "lft = scaler.transform(X4[:,:11])\n",
    "X4 = np.hstack((lft, X4[:,11][:,np.newaxis]))\n",
    "\n",
    "importer = Importer(vec, 20, 5)\n",
    "X56, y, groups_train = importer.import_from('train_groups.csv')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X56[:,:25])\n",
    "X56 = scaler.transform(X56[:,:25])\n",
    "\n",
    "importer = Importer(vec, 8, 3)\n",
    "X7, y, groups_train = importer.import_from('train_groups.csv')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X7)\n",
    "X7 = scaler.transform(X7)\n",
    "X7 = np.hstack((X7, X_train_SEV))\n",
    "X_TR = [X12, X3, X4, X56, X7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Регрессия показала себя намного хуже обычного голосования, при этом для неё ещё нужно было подбирать th.\n",
    "\n",
    "2) Пробовали делать миллиарды стеккингов с всевозможными внешними функциями, а также объединять в голосование их... Плохо... Переобучение огромное. Валидация выдавала 79.2-79.7 на всех 9 моделях. Голосование из них выдало 0.8. Но на каггле было 74.2. А времени убили колоссальное количество.\n",
    "\n",
    "3) Обычное голосование показало себя превосходно.\n",
    "\n",
    "4) Обычная нейронка с тангенсальной функцией активации тоже довольно стабильна и просто в исполнении + неплохой скор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Регрессия\n",
    "# class combined_cls:\n",
    "#     def __init__(self):\n",
    "# #         self.cls1 = LogisticRegression(solver='lbfgs', max_iter=1000, C=20.0) #все фичи (8, 3, 1, Seva)\n",
    "#         self.cls2 = LinearRegression() #все фичи (8, 3, 1, Seva)\n",
    "#         self.cls3 = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=0, n_jobs=-1) #26 соседей cos\n",
    "# #         self.cls4 = GaussianNB () #12 cos, 3 jac, dbscan\n",
    "#         self.cls5 = MLPClassifier(max_iter=800, hidden_layer_sizes=(10, 10), random_state=1) #20 соседей cos\n",
    "#         self.cls6 = MLPClassifier(max_iter=800, hidden_layer_sizes=(5, 5), random_state=1, activation='tanh') #20 соседей cos\n",
    "#         self.cls7 = SVC(C = 0.58,kernel = 'rbf', gamma='auto') #20 cos, 5 jac, Seva\n",
    "#         self.cls = LinearRegression()\n",
    "        \n",
    "#     def fit(self, X12, X3, X4, X56, X7, y):\n",
    "\n",
    "# #         self.cls1.fit(X12, y)\n",
    "#         self.cls2.fit(X12, y)\n",
    "#         self.cls3.fit(X3, y)\n",
    "# #         self.cls4.fit(X4, y)\n",
    "#         self.cls5.fit(X56, y)\n",
    "#         self.cls6.fit(X56, y)\n",
    "#         self.cls7.fit(X7, y)\n",
    "        \n",
    "#         self.cls.fit(np.array([\n",
    "# #             self.cls1.predict_proba(X12)[:, 1],\n",
    "#             self.cls2.predict(X12), \n",
    "#             self.cls3.predict_proba(X3)[:, 1], \n",
    "# #             self.cls4.predict_proba(X4)[:, 1],\n",
    "#             self.cls5.predict_proba(X56)[:, 1],\n",
    "#             self.cls6.predict_proba(X56)[:, 1],\n",
    "#             self.cls7.predict(X7)\n",
    "#         ]).T, y)\n",
    "        \n",
    "#     def predict(self, X12, X3, X4, X56, X7):\n",
    "#         return self.cls.predict(np.array([\n",
    "# #             self.cls1.predict_proba(X12)[:, 1],\n",
    "#             self.cls2.predict(X12), \n",
    "#             self.cls3.predict_proba(X3)[:, 1],\n",
    "# #             self.cls4.predict_proba(X4)[:, 1],\n",
    "#             self.cls5.predict_proba(X56)[:, 1],\n",
    "#             self.cls6.predict_proba(X56)[:, 1],\n",
    "#             self.cls7.predict(X7)\n",
    "#         ]).T)\n",
    "\n",
    "#74.5 на Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class Voiting(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        #self.cls1 = Linear(0.34) #все фичи (8, 3, 1, Seva)\n",
    "        self.cls2 = KNeighborsClassifier(42,weights='distance',p = 1)\n",
    "        self.cls3 = RandomForestClassifier(n_estimators=600, max_depth=10,criterion='entropy', random_state=0, n_jobs=-1) #26 соседей cos\n",
    "    #    self.cls4 = GaussianNB () #12 cos, 3 jac, dbscan\n",
    "        self.cls5 = MLPClassifier(max_iter=800, hidden_layer_sizes=(10, 10), random_state=1) #20 соседей cos\n",
    "        self.cls6 = MLPClassifier(max_iter=800, hidden_layer_sizes=(5, 5), random_state=1, activation='tanh') #20 соседей cos\n",
    "        self.cls7 = SVC(C = 483.2930238571752,kernel = 'rbf', gamma='auto') #20 cos, 5 jac, Seva\n",
    "     #   self.cls8 = GaussianProcessClassifier(n_jobs=-1)\n",
    "    def fit(self,X,y):\n",
    "        X12, X3, X4, X56, X7 = X\n",
    "        self.cls2.fit(X12, y)\n",
    "        self.cls3.fit(X3, y)\n",
    "       # self.cls4.fit(X4, y)\n",
    "        self.cls5.fit(X56, y)\n",
    "        self.cls6.fit(X56, y)\n",
    "        self.cls7.fit(X7, y)\n",
    "     #   self.cls8.fit(X7, y)\n",
    "    def predict(self,X):\n",
    "        X12, X3, X4, X56, X7 = X\n",
    "        self.pred = np.transpose(np.array([self.cls2.predict(X12),\n",
    "                              self.cls3.predict(X3),\n",
    "                                # self.cls4.predict(X4),\n",
    "                                self.cls5.predict(X56),\n",
    "                                self.cls6.predict(X56),\n",
    "                                self.cls7.predict(X7)]))#,\n",
    "                         #       self.cls8.predict(X7)]))\n",
    "        def pr(line):\n",
    "            ar,count = np.unique(line,return_counts = True)\n",
    "            return ar[np.argmax(count)]\n",
    "        predict = np.array([pr(line) for line in self.pred])\n",
    "        return predict    \n",
    "    \n",
    "#75.8 на Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(10, 80), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=800, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cls = MLPClassifier(max_iter=800, hidden_layer_sizes=(5, 5), random_state=1, activation='tanh')\n",
    "# cls.fit(X56, y_train_SEV)\n",
    "\n",
    "# predict = cls.predict(X56_test)\n",
    "\n",
    "# 75.2 на Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Voiting()\n",
    "model.fit(X_TR,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "importer = Importer(vec, 8, 3)\n",
    "X12, pair_ids = importer.import_from('test_groups.csv')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X12[:,:11])\n",
    "lft = scaler.transform(X12[:,:11])\n",
    "X12 = np.hstack((lft, X12[:,11][:,np.newaxis]))\n",
    "# X12 = np.hstack((X12, X_test_SEV))\n",
    "\n",
    "importer = Importer(vec, 26, 4)\n",
    "X3, pair_ids = importer.import_from('test_groups.csv')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X3[:,:30])\n",
    "X3 = scaler.transform(X3[:,:30])\n",
    "\n",
    "importer = Importer(vec, 8, 3)\n",
    "X4, pair_ids = importer.import_from('test_groups.csv')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X4[:,:11])\n",
    "lft = scaler.transform(X4[:,:11])\n",
    "X4 = np.hstack((lft, X4[:,11][:,np.newaxis]))\n",
    "\n",
    "importer = Importer(vec, 20, 5)\n",
    "X56, pair_ids = importer.import_from('test_groups.csv')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X56[:,:25])\n",
    "X56 = scaler.transform(X56[:,:25])\n",
    "\n",
    "importer = Importer(vec, 8, 3)\n",
    "X7, pair_ids = importer.import_from('test_groups.csv')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X7)\n",
    "X7 = scaler.transform(X7)\n",
    "X7 = np.hstack((X7, X_test_SEV))\n",
    "X_TE = [X12, X3, X4, X56, X7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(pair_ids, predict, filename='predict.csv'):\n",
    "    with open(filename, 'w') as f:\n",
    "        print('pair_id,target', file = f)\n",
    "        for pair_id, target in zip(pair_ids, predict):\n",
    "            print(pair_id, int(target), sep=',', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(X_TE)\n",
    "save_result(pair_ids, predict, 'predict.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
